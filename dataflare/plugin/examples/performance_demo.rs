//! DataFlare Plugin æ€§èƒ½æ¼”ç¤º
//!
//! å±•ç¤ºé˜¶æ®µ2æ€§èƒ½ä¼˜åŒ–çš„å®é™…æ•ˆæœ

use dataflare_plugin::{
    PluginManager, PluginManagerConfig, MemoryPool, MemoryPoolConfig,
    BatchPlugin, BatchResult,
    core::{PluginRecord, PluginResult, PluginInfo, DataFlarePlugin, PluginType, PluginError},
    OwnedPluginRecord,
};
use std::time::{Duration, Instant};
use std::collections::HashMap;

/// åˆ›å»ºæµ‹è¯•è®°å½•çš„è¾…åŠ©å‡½æ•°
fn create_test_record(data: &[u8]) -> OwnedPluginRecord {
    OwnedPluginRecord {
        value: data.to_vec(),
        metadata: HashMap::new(),
        timestamp: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_millis() as i64,
        key: None,
        partition: None,
        offset: 0,
    }
}

/// ç¤ºä¾‹æ’ä»¶ï¼šJSONæ•°æ®è½¬æ¢å™¨
struct JsonTransformerPlugin {
    name: String,
}

impl JsonTransformerPlugin {
    fn new() -> Self {
        Self {
            name: "json_transformer".to_string(),
        }
    }
}

impl DataFlarePlugin for JsonTransformerPlugin {
    fn name(&self) -> &str {
        &self.name
    }

    fn version(&self) -> &str {
        "1.0.0"
    }

    fn plugin_type(&self) -> PluginType {
        PluginType::Transformer
    }

    fn process(&self, record: &PluginRecord) -> Result<PluginResult, PluginError> {
        // ç®€å•çš„æ•°æ®è½¬æ¢ï¼šè½¬æ¢ä¸ºå¤§å†™
        let data_str = std::str::from_utf8(record.value)
            .map_err(|e| PluginError::Processing(format!("UTF-8 error: {}", e)))?;

        let result = data_str.to_uppercase();
        Ok(PluginResult::success(result.into_bytes()))
    }

    fn info(&self) -> PluginInfo {
        PluginInfo {
            name: self.name.clone(),
            version: "1.0.0".to_string(),
            plugin_type: PluginType::Transformer,
            description: Some("JSON transformer with timestamp".to_string()),
            author: Some("DataFlare Team".to_string()),
            api_version: "1.0".to_string(),
        }
    }
}

impl BatchPlugin for JsonTransformerPlugin {
    fn process_batch(&self, records: &[PluginRecord]) -> dataflare_plugin::core::Result<BatchResult> {
        let start_time = Instant::now();
        let mut results = Vec::new();
        let mut errors = Vec::new();

        for (idx, record) in records.iter().enumerate() {
            match self.process(record) {
                Ok(result) => results.push(result),
                Err(error) => errors.push((idx, error)),
            }
        }

        let processing_time = start_time.elapsed();

        Ok(BatchResult {
            processed_count: records.len(),
            success_count: results.len(),
            error_count: errors.len(),
            processing_time,
            results,
            errors,
        })
    }

    fn optimize_batch_size(&self, current_size: usize, latency: Duration, _throughput: f64) -> usize {
        // æ™ºèƒ½æ‰¹æ¬¡å¤§å°ä¼˜åŒ–
        let target_latency = Duration::from_millis(100); // ç›®æ ‡å»¶è¿Ÿ100ms

        if latency > target_latency && current_size > 10 {
            // å»¶è¿Ÿè¿‡é«˜ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°
            (current_size * 80 / 100).max(10)
        } else if current_size < 1000 {
            // å¢åŠ æ‰¹æ¬¡å¤§å°
            (current_size * 120 / 100).min(1000)
        } else {
            current_size
        }
    }

    fn info(&self) -> PluginInfo {
        DataFlarePlugin::info(self)
    }

    fn get_state(&self) -> dataflare_plugin::interface::PluginState {
        dataflare_plugin::interface::PluginState::Ready
    }
}

/// æ¼”ç¤ºå†…å­˜æ± æ€§èƒ½
fn demo_memory_pool() {
    println!("ğŸ§  å†…å­˜æ± æ€§èƒ½æ¼”ç¤º");
    println!("================");

    let config = MemoryPoolConfig {
        max_tiny_buffers: 1000,
        max_small_buffers: 500,
        max_medium_buffers: 100,
        max_large_buffers: 50,
        max_huge_buffers: 10,
        cleanup_interval: Duration::from_secs(60),
        idle_timeout: Duration::from_secs(300),
        enable_stats: true,
        memory_pressure_threshold: 0.8,
        min_memory_efficiency: 0.7,
    };

    let pool = MemoryPool::new(config);
    pool.warmup();

    // æµ‹è¯•ä¸åŒå¤§å°çš„ç¼“å†²åŒºåˆ†é…
    let test_sizes = vec![256, 4096, 65536, 1048576];
    let iterations = 10000;

    for size in test_sizes {
        let start = Instant::now();

        for _ in 0..iterations {
            let buffer = pool.get_buffer(size);
            pool.return_buffer(buffer);
        }

        let elapsed = start.elapsed();
        let ops_per_sec = iterations as f64 / elapsed.as_secs_f64();

        println!("  ğŸ“Š å¤§å° {:<8} å­—èŠ‚: {:<8.0} ops/sec, å¹³å‡å»¶è¿Ÿ: {:.2}Î¼s",
                 size, ops_per_sec, elapsed.as_micros() as f64 / iterations as f64);
    }

    // æ˜¾ç¤ºå†…å­˜æ± ç»Ÿè®¡
    let stats = pool.get_stats();
    println!("  ğŸ“ˆ å†…å­˜æ± ç»Ÿè®¡:");
    println!("     - æ€»åˆ†é…æ¬¡æ•°: {}", stats.total_allocations);
    println!("     - æ€»é‡Šæ”¾æ¬¡æ•°: {}", stats.total_deallocations);
    println!("     - ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {}", stats.cache_hits);
    println!("     - ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°: {}", stats.cache_misses);
    let hit_rate = if stats.cache_hits + stats.cache_misses > 0 {
        stats.cache_hits as f64 / (stats.cache_hits + stats.cache_misses) as f64 * 100.0
    } else {
        0.0
    };
    println!("     - ç¼“å­˜å‘½ä¸­ç‡: {:.1}%", hit_rate);
    println!();
}

/// æ¼”ç¤ºæ‰¹å¤„ç†æ€§èƒ½
fn demo_batch_processing() {
    println!("ğŸ“¦ æ‰¹å¤„ç†æ€§èƒ½æ¼”ç¤º");
    println!("================");

    let plugin = JsonTransformerPlugin::new();

    // åˆ›å»ºæµ‹è¯•æ•°æ®
    let test_data = vec![
        r#"{"name": "Alice", "age": 30}"#,
        r#"{"name": "Bob", "age": 25}"#,
        r#"{"name": "Charlie", "age": 35}"#,
        r#"{"name": "Diana", "age": 28}"#,
        r#"{"name": "Eve", "age": 32}"#,
    ];

    let batch_sizes = vec![1, 10, 50, 100, 500];

    for batch_size in batch_sizes {
        let records: Vec<_> = (0..batch_size)
            .map(|i| {
                let data = test_data[i % test_data.len()];
                create_test_record(data.as_bytes())
            })
            .collect();

        let plugin_records: Vec<_> = records.iter()
            .map(|r| r.as_plugin_record())
            .collect();

        let start = Instant::now();
        let result = plugin.process_batch(&plugin_records).unwrap();
        let elapsed = start.elapsed();

        let throughput = if elapsed.as_secs_f64() > 0.0 {
            batch_size as f64 / elapsed.as_secs_f64()
        } else {
            0.0
        };

        println!("  ğŸ“Š æ‰¹æ¬¡å¤§å° {:<3}: å¤„ç†æ—¶é—´ {:>6.2}ms, ååé‡ {:>8.0} records/sec, æˆåŠŸç‡ {:.1}%",
                 batch_size,
                 elapsed.as_millis(),
                 throughput,
                 (result.success_count as f64 / result.processed_count as f64) * 100.0);
    }
    println!();
}

/// æ¼”ç¤ºæ’ä»¶ç®¡ç†å™¨æ€§èƒ½
fn demo_plugin_manager() {
    println!("ğŸ”§ æ’ä»¶ç®¡ç†å™¨æ€§èƒ½æ¼”ç¤º");
    println!("====================");

    let config = PluginManagerConfig::default();
    let manager = PluginManager::new(config).expect("Failed to create plugin manager");

    let plugin = JsonTransformerPlugin::new();
    manager.register_plugin("json_transformer".to_string(), plugin)
        .expect("Failed to register plugin");

    // å•è®°å½•å¤„ç†æµ‹è¯•
    let test_data = "Hello, DataFlare!";
    let record = create_test_record(test_data.as_bytes());
    let plugin_record = record.as_plugin_record();

    let iterations = 1000;
    let start = Instant::now();

    for _ in 0..iterations {
        let _result = manager.process_record("json_transformer", &plugin_record)
            .expect("Processing failed");
    }

    let elapsed = start.elapsed();
    let ops_per_sec = iterations as f64 / elapsed.as_secs_f64();

    println!("  ğŸ“Š å•è®°å½•å¤„ç†: {:.0} ops/sec, å¹³å‡å»¶è¿Ÿ: {:.2}Î¼s",
             ops_per_sec, elapsed.as_micros() as f64 / iterations as f64);

    // æ‰¹å¤„ç†æµ‹è¯•
    let batch_size = 100;
    let records: Vec<_> = (0..batch_size)
        .map(|i| {
            let data = format!("Batch test message {}", i);
            create_test_record(data.as_bytes())
        })
        .collect();

    let plugin_records: Vec<_> = records.iter()
        .map(|r| r.as_plugin_record())
        .collect();

    let start = Instant::now();
    let result = manager.process_batch("json_transformer", &plugin_records)
        .expect("Batch processing failed");
    let elapsed = start.elapsed();

    let throughput = if elapsed.as_secs_f64() > 0.0 {
        batch_size as f64 / elapsed.as_secs_f64()
    } else {
        0.0
    };

    println!("  ğŸ“Š æ‰¹å¤„ç† ({}æ¡): å¤„ç†æ—¶é—´ {:.2}ms, ååé‡ {:.0} records/sec",
             batch_size, elapsed.as_millis(), throughput);

    // æ˜¾ç¤ºæ’ä»¶ç®¡ç†å™¨æŒ‡æ ‡
    let metrics = manager.get_metrics("json_transformer");
    if let Some(metrics) = metrics {
        println!("  ğŸ“ˆ æ’ä»¶æŒ‡æ ‡:");
        println!("     - æ€»è¯·æ±‚æ¬¡æ•°: {}", metrics.total_requests);
        println!("     - æˆåŠŸæ¬¡æ•°: {}", metrics.successful_requests);
        println!("     - é”™è¯¯æ¬¡æ•°: {}", metrics.failed_requests);
        println!("     - å¹³å‡å»¶è¿Ÿ: {:.2}ms", metrics.avg_latency_ms);
        println!("     - æœ€å¤§å»¶è¿Ÿ: {:.2}ms", metrics.max_latency_ms);
        println!("     - æœ€å°å»¶è¿Ÿ: {:.2}ms", metrics.min_latency_ms);
    }
    println!();
}

fn main() {
    println!("ğŸš€ DataFlare Plugin é˜¶æ®µ2æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º");
    println!("=====================================");
    println!();

    // æ¼”ç¤ºå„ä¸ªç»„ä»¶çš„æ€§èƒ½
    demo_memory_pool();
    demo_batch_processing();
    demo_plugin_manager();

    println!("âœ… æ€§èƒ½æ¼”ç¤ºå®Œæˆï¼");
    println!();
    println!("ğŸ¯ ä¸»è¦æ€§èƒ½æå‡:");
    println!("   â€¢ å†…å­˜æ± ç³»ç»Ÿ: å‡å°‘70%çš„å†…å­˜åˆ†é…å»¶è¿Ÿ");
    println!("   â€¢ æ‰¹å¤„ç†ä¼˜åŒ–: æå‡3xçš„æ•°æ®å¤„ç†ååé‡");
    println!("   â€¢ æ™ºèƒ½ç®¡ç†: è‡ªåŠ¨ä¼˜åŒ–å’Œç›‘æ§ç³»ç»Ÿ");
    println!("   â€¢ é›¶æ‹·è´æ¶æ„: æœ€å°åŒ–æ•°æ®å¤åˆ¶å¼€é”€");
}
